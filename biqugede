#!/usr/bin/env python
# vim:fileencoding=utf-8
from calibre.web.feeds.news import BasicNewsRecipe
from lxml.html import document_fromstring,tostring
class AdvancedUserRecipe(BasicNewsRecipe):
    title          = '重生地球仙尊'
    encoding = 'utf-8'
    # 抓取页面内容设置
    #///////////////////
    #keep_only_tags = [dict(id=['content', 'list']),dict(name='div', class_='bookname')] # 仅保留指定选择器包含的内容
    no_stylesheets = True # 去除 CSS 样式
    remove_javascript = True # 去除 JavaScript 脚本
    auto_cleanup = True # 自动清理 HTML 代码
    max_articles_per_feed = 9999 # 抓取文章数量
    simultaneous_downloads = 70
    ignore_duplicate_articles = {'title', 'url'}
    
    extra_css='''
            .calibre_navbar {
                font-family:monospace;
                height:0;
                display:none;
            }
    '''    
    
    #///////////////////
    # 页面列表解析方法
    #///////////////////
    def parse_index(self):
        print("1111111111111111111")
        bookId = '14532'
        orin = 'https://www.bige7.com'
        #orin = 'https://www.xbiquge.la'
        #site = orin + '/'+bookId[0:2] + '/' + bookId # 页面列表页
        site = orin + '/book/' + bookId +'/' # 页面列表页
        soup = self.index_to_soup(site) # 解析列表页返回 BeautifulSoup 对象
        links = soup.find_all(attrs={'class':'listmain'})[0].find_all('a') # 获取所有文章链接
        print('---------',site)
        self.cover_url = soup.find_all('meta',property='og:image',limit=1)[0].string
        articles = [] # 定义空文章资源数组
        for link in links: # 循环处理所有文章链接
            if link.get("rel") == "nofollow":
                continue
            title = link.contents[0].strip() # 提取文章标题
            url = orin + link.get("href") # 提取文章链接
            a = {'title': title , 'url':url} # 组合标题和链接
            articles.append(a) # 累加到数组中
        ans = [(self.title, articles)] # 组成最终的数据结构
        return ans # 返回可供 Calibre 转换的数据结构
    #解析内容方法
    def preprocess_raw_html(self, raw_html, url):
        soup = BeautifulSoup(raw_html)
        print("----------",raw_html)
        title = soup.find('h1').string
        content = soup.find(id='chaptercontent')
        #a = content.find('a')
        #a.clear()
        p = content.find('p')
        p.clear()
        #移除br标签
        #[s.extract() for s in content('br')]
        root = document_fromstring(
                '<html><head><title>%s</title></head><body>%s<body/></html>' %(title,content))
        raw_html = tostring(root, encoding='unicode')
        #print("html:",html)
        return raw_html
    
    
    def populate_article_metadatassssss(self, article, soup, first):
        #print("soup:",soup)
        tag = soup.find(id='content').find('p')
        print('p:',tag)
        tag.clear()
        pass
